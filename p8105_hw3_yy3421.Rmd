---
title: "p8105_hw3_yy3421"
output: github_document
---

## Problem 1
Loading dataset:

```{r}
library(tidyverse)
library(ggridges)
library(patchwork)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
library(p8105.datasets)
data("instacart")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Question 1:

Group by aisle:

```{r}
instacart_aisle = instacart |> 
  group_by(aisle)
```

```{r}
n_item_max = instacart_aisle |> 
  summarise(n_item=n()) |> 
  arrange(desc(n_item))
```

There are `r instacart_aisle |> summarize() |> nrow()` aisles. The most items are ordered from aisle "`r n_item_max$aisle[1]`" with `r n_item_max$n_item[1]` items.

### Question 2:
```{r}
n_item_max |> 
  filter(
    n_item > 10000
  ) |> 
  ggplot(aes(x= reorder(aisle, +n_item), y = n_item))+
  geom_point() +
  labs(
    y = "Aisle",
    x = "Number of Items",
    caption = "Number of Items for Each Aisle with More than 10,000 Items"
  )+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

### Question 3:
```{r}
instacart_aisle |> 
  group_by(aisle, product_name) |> 
  filter(
      aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits"
    ) |> 
  summarize(n_product = n()) |> 
  arrange(desc(n_product)) |> 
  mutate(
    product_rank = min_rank(desc(n_product))
    ) |> 
  filter(product_rank < 4) 

```

### Question 4:
```{r}
instacart |> 
  group_by(order_dow, product_name) |> 
  filter(
    product_name == "Pink Lady Apples"| product_name == "Coffee Ice Cream"
  ) |> 
  summarize(
    mean_hour = mean(order_hour_of_day)
  ) |> 
  mutate(
    order_dow = order_dow +1
  ) |> 
  pivot_wider(
    names_from = product_name,
    values_from = mean_hour
  )
```

## Problem 2

Load data and clean:

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

```{r}
brfss_smart2010 = 
  brfss_smart2010 |> 
  janitor::clean_names() |> 
  rename(state = locationabbr, county = locationdesc) |> 
  filter(
    topic == "Overall Health",
    response == "Excellent"|response == "Very good"|response == "Good"|response == "Fair"|response == "Poor"
  ) |> 
  mutate(
    response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
  ) |> 
  arrange(desc(response))
```

### Question 1:


```{r}
brfss_smart2002_state = 
  brfss_smart2010 |> 
  filter(
    year == 2002
  ) |> 
  group_by(state) |> 
  summarize(n_location = n_distinct(county)) |> 
  filter(
    n_location >= 7
  )

brfss_smart2010_state = 
  brfss_smart2010 |> 
  filter(
    year == 2010
  ) |> 
  group_by(state) |> 
  summarize(n_location = n_distinct(county)) |> 
  filter(
    n_location >= 7)
```

There are `r nrow(brfss_smart2002_state)` locations in 2002, which are CT, FL, MA, NC, NJ, PA.
There are `r nrow(brfss_smart2010_state)` locations in 2010, which are CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA.

### Question 2:

```{r}
brfss_smart2010 |> 
  filter(
    response == "Excellent"
  ) |> 
  select(year, state, data_value
  ) |> 
  group_by(state, year) |> 
  summarize(avg_data_value = mean(data_value, na.rm = TRUE)) |>
  ggplot(aes(
    x = year, 
    y = avg_data_value, 
    color = state,
    group = state))+
  geom_point()+
  geom_line()+
  labs(
    x = "Year",
    y = "Average Data Values",
    caption = "Change in Average Data Values from 2002 to 2010 for Each State"
  )
```


### Question 3:

```{r}
brfss_smart2010 |> 
  filter(
    state == "NY",
    year == 2006 | year == 2010
  ) |> 
  ggplot(
    aes(
      x = response, y = data_value, fill = response
    )
  )+
  geom_boxplot()+
  facet_grid(~year)+
  labs(
    x = "Response",
    y = "Data Value",
    caption = "Distribution of Data Value for Responses between 2006 and 2010"
  )
```


## Problem 3

Cleaning and merging the two csv:

```{r}
nhanes_accel = 
  read_csv("data/nhanes_accel.csv") |> 
  janitor::clean_names() 


nhanes_covar = 
  read_csv("data/nhanes_covar.csv", skip = 4) |> 
  janitor::clean_names() |> 
  filter(
    age>=21,
    is.na(bmi) == FALSE,
    is.na(education) == FALSE
  ) |> 
   mutate(
    sex = 
      case_match(
        sex,
        1 ~ "male",
        2 ~ "female"),
    sex = as.factor(sex),
    education =
      case_match(
        education,
        1 ~ "less than high school",
        2 ~ "high school equivalent",
        3 ~ "more than high school"),
    education = factor(education, levels = c("less than high school", "high school equivalent", "more than high school")))

nhanes_df = 
  left_join(nhanes_covar, nhanes_accel, by = join_by(seqn))
```

### Question 1: 
Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. 

Table:
```{r}
nhanes_covar |> 
  select(education, sex) |> 
  table()

```

Visualization of Age Distribution:
```{r}
nhanes_df |> 
  group_by(education) |> 
  ggplot(
    aes(
      y = age,
      color = sex
    )
  )+
  geom_boxplot()+
  facet_grid(~ education)
```


### Question 2:
Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. Comment on your plot.

```{r}

```

### Question 3:
Accelerometer data allows the inspection activity over the course of the day. Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based on this graph; including smooth trends may help identify differences.


